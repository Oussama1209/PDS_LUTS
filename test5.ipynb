{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"from lightglue import LightGlue, SuperPoint, viz2d, DISK, SIFT, ALIKED, DoGHardNet\\n\",\n",
    "    \"from lightglue.utils import load_image, rbd, match_pair\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import cv2 as cv\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import torchvision.transforms.functional as TF\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import cv2\\n\",\n",
    "    \"import pandas as pd  # Import pandas\\n\",\n",
    "    \"import glob\\n\",\n",
    "    \"import math\\n\",\n",
    "    \"from tqdm.notebook import tqdm\\n\",\n",
    "    \"\\n\",\n",
    "    \"general_folder_path = '/home/oussama/Documents/EPFL/PDS_LUTS/'\\n\",\n",
    "    \"\\n\",\n",
    "    \"output_path = general_folder_path + 'panorama.jpg'\\n\",\n",
    "    \"\\n\",\n",
    "    \"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n\",\n",
    "    \"\\n\",\n",
    "    \"torch.cuda.empty_cache()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Utility function to calculate image corners in homogeneous coordinates\\n\",\n",
    "    \"def get_homogeneous_corners(width, height):\\n\",\n",
    "    \"    return np.array([[0, 0, 1], [width, 0, 1], [width, height, 1], [0, height, 1]]).T\\n\",\n",
    "    \"\\n\",\n",
    "    \"def warp_perspective_padded1(src, dst, transf):\\n\",\n",
    "    \"    src_h, src_w = src.shape[:2]\\n\",\n",
    "    \"    dst_h, dst_w = dst.shape[:2]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Define the corners of the src image\\n\",\n",
    "    \"    src_corners = np.array([\\n\",\n",
    "    \"        [0, 0],\\n\",\n",
    "    \"        [src_w, 0],\\n\",\n",
    "    \"        [src_w, src_h],\\n\",\n",
    "    \"        [0, src_h]\\n\",\n",
    "    \"    ], dtype=np.float32)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Transform the src corners using the homography matrix (transf)\\n\",\n",
    "    \"    src_corners_transformed = cv2.perspectiveTransform(src_corners[None, :, :], transf)[0]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Define the corners of the dst image in its own coordinate space\\n\",\n",
    "    \"    dst_corners = np.array([\\n\",\n",
    "    \"        [0, 0],\\n\",\n",
    "    \"        [dst_w, 0],\\n\",\n",
    "    \"        [dst_w, dst_h],\\n\",\n",
    "    \"        [0, dst_h]\\n\",\n",
    "    \"    ], dtype=np.float32)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Combine all corners to find the overall bounding box\\n\",\n",
    "    \"    all_corners = np.vstack((src_corners_transformed, dst_corners))\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Compute the bounding box of all corners\\n\",\n",
    "    \"    x_min, y_min = np.int32(all_corners.min(axis=0))\\n\",\n",
    "    \"    x_max, y_max = np.int32(all_corners.max(axis=0))\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Calculate the translation needed to shift images to positive coordinates\\n\",\n",
    "    \"    shift_x = -x_min\\n\",\n",
    "    \"    shift_y = -y_min\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Compute the size of the output canvas\\n\",\n",
    "    \"    output_width = x_max - x_min\\n\",\n",
    "    \"    output_height = y_max - y_min\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Compute the 3x3 translation matrix to shift the images\\n\",\n",
    "    \"    translation_matrix = np.array([\\n\",\n",
    "    \"        [1, 0, shift_x],\\n\",\n",
    "    \"        [0, 1, shift_y],\\n\",\n",
    "    \"        [0, 0,      1]\\n\",\n",
    "    \"    ], dtype=np.float32)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Update the transformation matrix to include the translation\\n\",\n",
    "    \"    new_transf = translation_matrix @ transf\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Warp the src image using the updated transformation matrix\\n\",\n",
    "    \"    warped = cv2.warpPerspective(src, new_transf, (output_width, output_height))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Warp the dst image using only the translation matrix (affine)\\n\",\n",
    "    \"    dst_pad = cv2.warpAffine(dst, translation_matrix[:2], (output_width, output_height))\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Determine the anchor points\\n\",\n",
    "    \"    anchorX = int(shift_x)\\n\",\n",
    "    \"    anchorY = int(shift_y)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Determine the sign\\n\",\n",
    "    \"    sign = (anchorX > 0) or (anchorY > 0)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    return dst_pad, warped, anchorX, anchorY, sign\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Rotate image tensor by specified angle\\n\",\n",
    "    \"def rotate_image(image, angle):\\n\",\n",
    "    \"    return TF.rotate(image, angle)\\n\",\n",
    "    \"\\n\",\n",
    "    \"def rotate_image1(image, angle):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Rotate a PyTorch image tensor without cropping and add necessary padding to preserve all content.\\n\",\n",
    "    \"    Keeps the tensor on its original device (CPU or CUDA).\\n\",\n",
    "    \"    Args:\\n\",\n",
    "    \"        image: A PyTorch tensor in CHW format with values in [0, 1].\\n\",\n",
    "    \"        angle: Rotation angle in degrees (counterclockwise).\\n\",\n",
    "    \"    Returns:\\n\",\n",
    "    \"        Rotated image as a PyTorch tensor on the same device.\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    device = image.device  # Store original device (CPU or CUDA)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Convert tensor to NumPy array (HWC format)\\n\",\n",
    "    \"    if isinstance(image, torch.Tensor):\\n\",\n",
    "    \"        image_np = image.permute(1, 2, 0).cpu().numpy()  # CHW -> HWC on CPU\\n\",\n",
    "    \"        if image_np.max() <= 1:  # Scale up to [0, 255] if needed\\n\",\n",
    "    \"            image_np = (image_np * 255).astype(np.uint8)\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        raise TypeError(\\\"Input must be a PyTorch tensor in CHW format.\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Get the height and width of the image\\n\",\n",
    "    \"    h, w = image_np.shape[:2]\\n\",\n",
    "    \"    center = (w // 2, h // 2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Compute the rotation matrix and new dimensions\\n\",\n",
    "    \"    rotation_matrix = cv.getRotationMatrix2D(center, angle, 1.0)\\n\",\n",
    "    \"    cos_val = abs(rotation_matrix[0, 0])\\n\",\n",
    "    \"    sin_val = abs(rotation_matrix[0, 1])\\n\",\n",
    "    \"    new_w = int((h * sin_val) + (w * cos_val))\\n\",\n",
    "    \"    new_h = int((h * cos_val) + (w * sin_val))\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Adjust the rotation matrix to account for translation\\n\",\n",
    "    \"    rotation_matrix[0, 2] += (new_w / 2) - center[0]\\n\",\n",
    "    \"    rotation_matrix[1, 2] += (new_h / 2) - center[1]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Perform rotation with padding\\n\",\n",
    "    \"    rotated_image_np = cv.warpAffine(\\n\",\n",
    "    \"        image_np, rotation_matrix, (new_w, new_h),\\n\",\n",
    "    \"        flags=cv.INTER_CUBIC,\\n\",\n",
    "    \"        borderMode=cv.BORDER_CONSTANT,\\n\",\n",
    "    \"        borderValue=(0, 0, 0)\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Convert back to PyTorch tensor (CHW format)\\n\",\n",
    "    \"    rotated_image_tensor = torch.from_numpy(rotated_image_np).permute(2, 0, 1).float()\\n\",\n",
    "    \"    if rotated_image_tensor.max() > 1:  # Normalize back to [0, 1]\\n\",\n",
    "    \"        rotated_image_tensor /= 255.0\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Move the tensor back to the original device\\n\",\n",
    "    \"    return rotated_image_tensor.to(device)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Calculate similarity score between two images\\n\",\n",
    "    \"def compute_similarity_score(image1, image2, extractor, matcher):\\n\",\n",
    "    \"    with torch.no_grad():\\n\",\n",
    "    \"        feats0 = extractor.extract(image1)\\n\",\n",
    "    \"        feats1 = extractor.extract(image2)\\n\",\n",
    "    \"        feats0, feats1, matches01 = match_pair(extractor, matcher, image1, image2)\\n\",\n",
    "    \"    points0 = feats0['keypoints'][matches01['matches'][..., 0]]\\n\",\n",
    "    \"    score = points0.shape[0]\\n\",\n",
    "    \"    # Release GPU memory\\n\",\n",
    "    \"    del feats0, feats1, matches01, points0\\n\",\n",
    "    \"    torch.cuda.empty_cache()\\n\",\n",
    "    \"    return score\\n\",\n",
    "    \"\\n\",\n",
    "    \"def split_image_paths(image_paths):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Splits the image_paths list into two halves:\\n\",\n",
    "    \"    - First half gets an extra frame if the total number of images is odd.\\n\",\n",
    "    \"    - Second half is reversed to start from the last image and go to the middle.\\n\",\n",
    "    \"\\n\",\n",
    "    \"    Args:\\n\",\n",
    "    \"        image_paths (list): List of image paths.\\n\",\n",
    "    \"\\n\",\n",
    "    \"    Returns:\\n\",\n",
    "    \"        tuple: (first_half, second_half) - two lists of image paths.\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    # Compute the midpoint\\n\",\n",
    "    \"    mid = (len(image_paths) + 1) // 2\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Split the list\\n\",\n",
    "    \"    first_half = image_paths[:mid]  # First half\\n\",\n",
    "    \"    second_half = image_paths[mid:][::-1]  # Second half, reversed\\n\",\n",
    "    \"\\n\",\n",
    "    \"    return first_half, second_half\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Feature extractor and matcher initialization\\n\",\n",
    "    \"extractor = DoGHardNet(max_num_keypoints=None).eval().cuda()\\n\",\n",
    "    \"matcher = LightGlue(features='doghardnet').eval().cuda()\\n\",\n",
    "    \"\\n\",\n",
    "    \"start = True\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
